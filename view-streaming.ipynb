{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - View Streaming\n",
    "\n",
    "Este material auxilia o passo-a-passo na cria√ß√£o do ambiente necess√°rio para simular a auto-atualiza√ß√£o das Views Materializadas. Siga os passos a seguir para simular o exemplo da Video-Aula de Materialized Views.\n",
    "\n",
    "1. **Crie um projeto** em seu GCP;\n",
    "2. **Cria uma conta de servi√ßo** para acesso ao BigQuery [https://console.cloud.google.com/iam-admin/serviceaccounts];\n",
    "3. **Exporte o arquivo Json da chave** para o mesmo diret√≥rio que se encontra este arquivo;\n",
    "4. Execute os **passo de 1 a 6** deste tutorial;\n",
    "5. Abra o arquivo `scripts-carga-bq.sql`\n",
    "6. Substistua no arquivo `scripts-carga-bq.sql` o texto `nome-projeto` pelo nome √∫nico do seu projeto criado no passo 1;\n",
    "7. Execute o SQL do arquivo `scripts-carga-bq.sql` para gerar dados aleat√≥rios nas tabelas `AIRLINE_FLIGHTS` e `COMPANY_SALES`\n",
    "\n",
    "\n",
    "   \n",
    "_Rubens Mussi Cury_     \n",
    "ü•áGoogle Cloud Certified   \n",
    "rubensmussicury@gmail.com   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bibliotecas\n",
    "* Conta de Servi√ßo - https://cloud.google.com/docs/authentication/production\n",
    "* BigQuery - https://cloud.google.com/bigquery/docs/how-to e https://google-cloud-python.readthedocs.io/en/0.32.0/bigquery/usage.html\n",
    "* Exce√ß√µes - https://google-cloud-python.readthedocs.io/en/0.32.0/core/exceptions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from google.api_core.exceptions import BadRequest, Conflict, AlreadyExists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fun√ß√µes\n",
    "- Gera data aleat√≥ria para auxiliar na ingest√£o de dados\n",
    "- Cria conjunto de dados do tamanho e tipo solicitado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cria tabelas de acordo com a rela√ß√£o.\n",
    "tables       Array no formato [{\"table_id\": \"\", \"schema\": []}]\n",
    "  \n",
    "return       Mensagem informando o resultado da cria√ß√£o da tabela.\n",
    "\"\"\"\n",
    "def create_tables(tables):    \n",
    "    \n",
    "    # Percorre todas as tabelas definidas para cria√ß√£o.\n",
    "    for table in tables:        \n",
    "        \n",
    "        # Captura o a identifica√ß√£o da tabela e o schema.\n",
    "        table_id = table[\"table_id\"]\n",
    "        schema = table[\"schema\"]\n",
    "        \n",
    "        # Instancia o objeto da tabela.\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "\n",
    "        try:\n",
    "            # Executa o job de cria√ß√£o do Dataset.\n",
    "            executed_job = bq_client.create_table(table)\n",
    "            print(\"Tabela criada - {0}\".format(table_id))\n",
    "\n",
    "        # Conflito na cria√ß√£o do Dataset.\n",
    "        except Conflict as e:\n",
    "            print(\"Erro ao criar tabela - {0}\".format(table_id))\n",
    "            print(\"/n\".join(e.args))\n",
    "\n",
    "        # Exce√ß√£o gen√©rica.\n",
    "        except Exception as e:\n",
    "            print(\"Erro ao criar tabela - {0}\".format(table_id))\n",
    "            print(\"/n\".join(e.args))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Gera uma data aleat√≥ria dentro de um intervalo de datas.\n",
    "start_date   Data m√≠nima do intervalo.\n",
    "end_date     Data m√°xima do intervalo.\n",
    "\n",
    "return       Uma data aleat√≥ria entre [start_date] e [end_date]\n",
    "\"\"\"\n",
    "def random_date(start_date, end_date):\n",
    "    time_between_dates = end_date - start_date\n",
    "    days_between_dates = time_between_dates.days\n",
    "    random_number_of_days = random.randrange(days_between_dates)\n",
    "    return start_date + datetime.timedelta(days=random_number_of_days)\n",
    "\n",
    "\"\"\"\n",
    "Cria conjunto de dados do tamanho e tipo solicitado.\n",
    "data_type    [AIRLINE_FLIGHTS] ou [COMPANY_SALES]\n",
    "batch_size   Tamanho do conjunto de dados\n",
    "\n",
    "return       Dicion√°rio com os dados correspondentes ao data_type.\n",
    "\"\"\"\n",
    "def generate_random_data(data_type, batch_size):\n",
    "    batch_data = []\n",
    "    \n",
    "    if \"AIRLINE_FLIGHTS\" in data_type:\n",
    "        \n",
    "        for row in range(batch_size):\n",
    "\n",
    "            year = random.randint(2010, 2019)\n",
    "            month = random.randint(1, 12)\n",
    "            hour = random.randint(1, 24)\n",
    "            airline = [\"Southwest Airlines Co.: WN\", \"American Airlines Inc.: AA\", \"Delta Air Lines Inc.: DL\", \"SkyWest Airlines Inc.: OO\", \"United Air Lines Inc.: UA\", \"American Eagle Airlines Inc.: MQ\", \"US Airways Inc.: US\", \"Northwest Airlines Inc.: NW\", \"ExpressJet Airlines Inc.: XE\", \"ExpressJet Airlines Inc.: EV\"]\n",
    "            flights = int(random.random() * 100)\n",
    "\n",
    "            batch_data.append({\"YEAR\": year,\n",
    "                               \"MONTH\": month,\n",
    "                               \"HOUR\": hour,\n",
    "                               \"AIRLINE\": airline[random.randint(0, len(airline) - 1)],\n",
    "                               \"FLIGHTS\": flights})\n",
    "    if \"COMPANY_SALES\" in data_type:\n",
    "        \n",
    "        for row in range(batch_size):\n",
    "\n",
    "            date = random_date(datetime.date(2010, 1, 1), datetime.date.today()).isoformat()\n",
    "            company = [\"Progressive Sports\", \"Country Parts Shop\", \"Bikes and Motorbikes\", \"Budget Toy Store\", \"Advanced Bike Components\", \"Bicycle Warehouse Inc.\"]\n",
    "            sales = round(random.random() * 1000, 2)\n",
    "\n",
    "            batch_data.append({\"DATE\": date,\n",
    "                               \"COMPANY\": company[random.randint(0, len(company) - 1)],\n",
    "                               \"SALES\": sales})            \n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autentica√ß√£o\n",
    "* https://cloud.google.com/iam/docs/creating-managing-service-account-keys?hl=pt-br\n",
    "* Via `os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]`\n",
    "* Via service_account.Credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho completo da chave de servi√ßo\n",
    "gcp_credential_file = \"sua-chave-aqui.json\"\n",
    "\n",
    "# Autentica√ß√£o na conta de servi√ßo.\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = gcp_credential_file\n",
    "\n",
    "# Utilize esta forma de credencial para autenticar diretamente pelo SDK.\n",
    "# credentials = service_account.Credentials.from_service_account_file(gcp_credential_file)\n",
    "# client = bigquery.Client(credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ambiente\n",
    "- Identifica√ß√£o do Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o Client do BQ.\n",
    "bq_client = bigquery.Client()\n",
    "\n",
    "# Obt√©m nome √∫nico global do projeto vinculado a conta de servi√ßo.\n",
    "project_id = bq_client.project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Datasets\n",
    "- Cri√ß√£o do Datasets - https://cloud.google.com/bigquery/docs/datasets#python\n",
    "\n",
    "Criaremos Datasets `primary_dataset` e `secondary_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define os nomes dos datasets a serem criados no projeto.\n",
    "dataset_ids = [\"{0}.{1}\".format(project_id, \"primary_dataset\"), \"{0}.{1}\".format(project_id, \"secondary_dataset\")]\n",
    "\n",
    "# Cria cada um dos Datasets definidos.\n",
    "for dataset_id in dataset_ids:\n",
    "    \n",
    "    # Instancia o objeto do Dataset.\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "    # Define a localiza√ß√£o do Dataset.\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    try:\n",
    "        # Executa o job de cria√ß√£o do Dataset.\n",
    "        executed_job = bq_client.create_dataset(dataset)\n",
    "        print(\"Dataset criado - {0}.{1}\".format(project_id, dataset_id))\n",
    "    \n",
    "    # Conflito na cria√ß√£o do Dataset.\n",
    "    except Conflict as e:\n",
    "        print(\"Erro ao criar dataset - {0}.{1}\".format(project_id, dataset_id))\n",
    "        print(\"/n\".join(e.args))\n",
    "    \n",
    "    # Exce√ß√£o gen√©rica.\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao criar dataset - {0}.{1}\".format(project_id, dataset_id))\n",
    "        print(\"/n\".join(e.args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tabelas\n",
    "- Cria√ß√£o das Tabelas - https://cloud.google.com/bigquery/docs/tables#python\n",
    "\n",
    "Criaremos as tabelas `primary_dataset.COMPANY_SALES` e `secondary_dataset.AIRLINE_FLIGHTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define as especifica√ß√µes das tabelas [AIRLINE_FLIGHTS] e [COMPANY_SALES]\n",
    "tables = [{\"table_id\": \"{0}.{1}\".format(dataset_ids[0], \"AIRLINE_FLIGHTS\"),\n",
    "           \"schema\": [\n",
    "               bigquery.SchemaField(\"YEAR\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "               bigquery.SchemaField(\"MONTH\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "               bigquery.SchemaField(\"HOUR\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "               bigquery.SchemaField(\"AIRLINE\", \"STRING\", mode=\"REQUIRED\"),\n",
    "               bigquery.SchemaField(\"FLIGHTS\", \"INTEGER\", mode=\"REQUIRED\")\n",
    "           ]},\n",
    "          {\"table_id\": \"{0}.{1}\".format(dataset_ids[1], \"COMPANY_SALES\"),\n",
    "           \"schema\": [\n",
    "               bigquery.SchemaField(\"DATE\", \"DATE\", mode=\"REQUIRED\"),\n",
    "               bigquery.SchemaField(\"COMPANY\", \"STRING\", mode=\"REQUIRED\"),\n",
    "               bigquery.SchemaField(\"SALES\", \"FLOAT\", mode=\"REQUIRED\")\n",
    "           ]},          \n",
    "         ]\n",
    "\n",
    "# Cria as tabelas de acordo com as especifica√ß√µes.\n",
    "create_tables(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Streaming\n",
    "- Incrementa batchs de 800 registros nas tabelas `AIRLINE_FLIGHTS` e `COMPANY_SALES`\n",
    "- Utilize este passo para simular a atualiza√ß√£o das Views Materializadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de batchs a ser carregados nas tabelas.\n",
    "total_batchs = 50000\n",
    "batch_size = 800\n",
    "\n",
    "# Processo de ingest√£o.\n",
    "print(\"Iniciado processo de streaming...\")\n",
    "print(\"Tam. da Carga   - {}\".format(batch_size))\n",
    "print(\"Total de Linhas - {}\".format(total_batchs * batch_size))\n",
    "\n",
    "for batch in range(total_batchs):\n",
    "    \n",
    "    for table in tables:\n",
    "        \n",
    "        # Caputura o Id da tabela.\n",
    "        table_id = table[\"table_id\"] \n",
    "        \n",
    "        # Gera 800 registros aleat√≥rios.\n",
    "        batch_data = generate_random_data(table_id, batch_size)\n",
    "        \n",
    "        # Obt√©m o objeto da tabela que receber√° os dados.\n",
    "        table_object = bq_client.get_table(table_id)\n",
    "        \n",
    "        # Insere os registros no formato JSON.\n",
    "        executed_job = bq_client.insert_rows_json(table_object, batch_data)\n",
    "        \n",
    "        if executed_job == []:\n",
    "            # Atualiza percentual de ingest√£o.\n",
    "            percentage_loaded = int((batch * batch_size) / (total_batchs * batch_size) * 100)\n",
    "            print(\"{0} % conclu√≠do - {1} j√° linhas inseridas na tabela {2}\".format(percentage_loaded, \n",
    "                                                                                  batch * batch_size, \n",
    "                                                                                  table_id), end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
